{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e6965d",
   "metadata": {},
   "source": [
    "原论文地址：https://www.researchgate.net/publication/323419312\n",
    "\n",
    "我不是人工智能专业的，大学也仅仅开了《机器学习》和《人工智能》选修课，并没有接触过比较深的课程，但是我对这个领域非常好奇，最近找到了唐老师的一篇论文，用所学的知识进行了简单的复现。\n",
    "\n",
    "由于理论知识的缺乏和对pytorch掌握不够，其中的随机池化（stochastic pooling），目前我无法实现，只能使用torch自带的模块进行模型的搭建。\n",
    "\n",
    "\n",
    "### 开发环境\n",
    "由于本科是做大数据的，经常需要集群，一套大数据服务下来内存动不动占用20G+，所以内存直接给到了32G，但是测试跑深度模型内存占不了太多，比较影响训练速度的是显卡，其中我的显卡3060 Laptop，功耗130W，6G显存，3840个cuda核心，算力49左右，跑模型足够用。\n",
    "* 操作系统：win11\n",
    "* cpu：i7-11800H\n",
    "* 显卡：3060 Laptop（6G）\n",
    "* 内存：32G\n",
    "* cuda版本：11.6\n",
    "* python版本：3.6.9\n",
    "* pytorch版本：1.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1535920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from data_augmentation import data_enhance_rotate, data_enhance_gamma\n",
    "# 使用GPU训练，3060 Laptop显卡，30个epoch一共需要不到半小时\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e219e",
   "metadata": {},
   "source": [
    "### 制作数据集\n",
    "没有原文茶叶数据集，所以我就在kaggle上搜索了相似的花分类数据集\n",
    "\n",
    "kaggle地址在：https://www.kaggle.com/alxmamaev/flowers-recognition?select=flowers\n",
    "\n",
    "其中有五类花共4317个图片，只采用了3类，只选取900张图片\n",
    "\n",
    "其中训练集共有300张，三类分别各有100张\n",
    "\n",
    "测试集共有600张，三类各有200张\n",
    "\n",
    "经过数据增强（角度变换9000张 + 伽马纠正9000张 + 原始数据300张）后，训练集共有18300张 （原文没有用原始300张数据，并且原文用了五个数据增强的方法）\n",
    "\n",
    "数据增强函数在[data_augmentation.py](./data_augmentation.py)中\n",
    "\n",
    "同时对图片应用了归一化和缩放为256x256大小，至于三个通道的均值和标准差为什么用[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]，参考了网上的回答\n",
    "https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2\n",
    "\n",
    "数据集目录树：\n",
    "\n",
    "    datasets\n",
    "\n",
    "    +---test_data\n",
    "    \n",
    "    |   +---daisy\n",
    "    \n",
    "    |   +---dandelion\n",
    "    \n",
    "    |   \\---rose\n",
    "    \n",
    "    \\---train_data\n",
    "    \n",
    "        +---daisy\n",
    "        \n",
    "        +---dandelion\n",
    "        \n",
    "        \\---rose\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8375aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_train:  300\n",
      "train_loader:  18300\n",
      "test_loader:  600\n"
     ]
    }
   ],
   "source": [
    "data_dir = './datasets'\n",
    "BATCH_SIZE = 256  # 256实测占用4-5G显存\n",
    "data_transforms = {\n",
    "    'train_data': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 均值，标准差\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 均值，标准差\n",
    "    ]),\n",
    "    'test_data': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in\n",
    "                  ['train_data', 'test_data']}\n",
    "# 300张原始数据，用于后面验证训练集\n",
    "origin_train_loader = torch.utils.data.DataLoader(image_datasets.get(\"train_data\"), batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"origin_train: \", len(origin_train_loader.dataset))\n",
    "# 旋转图片，从-15度到15度，每次递增1，跳过0度，一共产生9000张\n",
    "image_datasets['train_data'] += data_enhance_rotate(data_dir)\n",
    "# 伽马纠正，从0.7到1.3，每次递增0.02，总共30次，一共产生9000张\n",
    "image_datasets['train_data'] += data_enhance_gamma(data_dir)\n",
    "train_loader = torch.utils.data.DataLoader(image_datasets.get(\"train_data\"), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(image_datasets.get(\"test_data\"), batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader一共18300数据，test_loader一共600数据\n",
    "print(\"train_loader: \", len(train_loader.dataset))\n",
    "print(\"test_loader: \", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4683fa8",
   "metadata": {},
   "source": [
    "### 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0e7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./logs\")\n",
    "# 查看第一批的数据集，总共256张，（256, 3, 256, 256），其中图片顺序已打乱\n",
    "for imgs, labels in train_loader:\n",
    "    writer.add_images(\"imgs\", imgs)\n",
    "    break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3bfd67",
   "metadata": {},
   "source": [
    "<img src=\"./images/images_show.png\" alt=\"images_show\" style=\"zoom: 80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cd83d",
   "metadata": {},
   "source": [
    "### 模型搭建\n",
    "按照原文进行模型搭建，随机池化我没有实现，就用torch自带的最大池化代替了\n",
    "\n",
    "其中有五层卷积和5层池化层，还有两层全连接，激活函数使用relu，最后使用softmax求概率\n",
    "\n",
    "模型一共1,627,563个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4411acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python_venv\\torch_venv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 40, 86, 86]          1,120\n",
      "├─Conv2d: 1-2                            [-1, 80, 28, 28]          80,080\n",
      "├─Conv2d: 1-3                            [-1, 120, 10, 10]         86,520\n",
      "├─Conv2d: 1-4                            [-1, 120, 10, 10]         129,720\n",
      "├─Conv2d: 1-5                            [-1, 120, 10, 10]         129,720\n",
      "├─Linear: 1-6                            [-1, 100]                 1,200,100\n",
      "├─Dropout: 1-7                           [-1, 100]                 --\n",
      "├─Linear: 1-8                            [-1, 3]                   303\n",
      "==========================================================================================\n",
      "Total params: 1,627,563\n",
      "Trainable params: 1,627,563\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 106.47\n",
      "==========================================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 3.01\n",
      "Params size (MB): 6.21\n",
      "Estimated Total Size (MB): 9.97\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 40, 86, 86]          1,120\n",
       "├─Conv2d: 1-2                            [-1, 80, 28, 28]          80,080\n",
       "├─Conv2d: 1-3                            [-1, 120, 10, 10]         86,520\n",
       "├─Conv2d: 1-4                            [-1, 120, 10, 10]         129,720\n",
       "├─Conv2d: 1-5                            [-1, 120, 10, 10]         129,720\n",
       "├─Linear: 1-6                            [-1, 100]                 1,200,100\n",
       "├─Dropout: 1-7                           [-1, 100]                 --\n",
       "├─Linear: 1-8                            [-1, 3]                   303\n",
       "==========================================================================================\n",
       "Total params: 1,627,563\n",
       "Trainable params: 1,627,563\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 106.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 3.01\n",
       "Params size (MB): 6.21\n",
       "Estimated Total Size (MB): 9.97\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    原文中的网络模型，不过随机池化没有实现\n",
    "\"\"\"\n",
    "class CNN_SP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小\n",
    "        self.conv1 = nn.Conv2d(3, 40, 3, stride=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(40, 80, 5, stride=3, padding=0)\n",
    "        self.conv3 = nn.Conv2d(80, 120, 3, stride=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(120, 120, 3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(120, 120, 3, stride=1, padding=1)\n",
    "        # 原文中dropout的比率为0.1，防止过拟合\n",
    "        self.dropout_layer = torch.nn.Dropout(0.1)\n",
    "        # 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数\n",
    "        self.fc1 = nn.Linear(120 * 10 * 10, 100)\n",
    "        self.fc2 = nn.Linear(100, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool5(out)\n",
    "        out = out.view(in_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out, dim=1)  # 计算log(softmax(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "# model = StochasticPooling().to(DEVICE)\n",
    "model = CNN_SP().to(DEVICE)\n",
    "# summary(model, (40, 86, 86))\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d2d16",
   "metadata": {},
   "source": [
    "### 自适应学习率，优化器的配置\n",
    "* 学习率：开始0.01，每10个epch就将lr减少10倍，测试了每3个epoch减少一次lr比固定lr准确率高了5%，但是10个epoch减少一次lr不知为何准确率上不去\n",
    "* 优化器：相比较SGDM与Adam，Adam拟合速度比较快，但最终结果差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ccbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim(model):\n",
    "    start_lr = 0.01\n",
    "    # 原文使用的SGD，动量为0.9，可以跟Adam进行比较\n",
    "    optimizer = optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=start_lr)\n",
    "    return optimizer\n",
    "'''\n",
    "    自适应学习率，复现原文中的每10个epoch将学习率减少10倍\n",
    "'''\n",
    "def adjust_learning_rate(optimizer, epoch, start_lr):\n",
    "    lr = start_lr * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7373f0",
   "metadata": {},
   "source": [
    "### 训练与测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60568909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    # 自适应学习率\n",
    "    start_lr = 0.01\n",
    "    adjust_learning_rate(optimizer, epoch, start_lr)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # 原文使用了cross_entropy损失函数，可以与nll_loss进行比较\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        # loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 每10个batch后就打印下信息，并写入tensorboard\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tLr:{:.2E}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item(),\n",
    "                optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "            writer.add_scalar('train_loss', loss.item(), (epoch - 1) * len(train_loader) + batch_idx)\n",
    "    train_val(model, device, origin_train_loader, epoch, writer)\n",
    "def train_val(model, device, train_loader, epoch, writer):\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            train_loss += F.cross_entropy(output, target, reduction='sum').item()  # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        train_loss, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "    writer.add_scalar('train_acc', 100. * correct / len(train_loader.dataset), epoch)\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch, writer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    writer.add_scalar('test_acc', 100. * correct / len(test_loader.dataset), epoch)\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d46419",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9339093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(epochs, model, device, train_loader):\n",
    "    # tensorboard, 记录loss和acc\n",
    "    writer = SummaryWriter(\"./logs\")\n",
    "    optimizer = get_optim(model)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, writer)\n",
    "        test(model, device, test_loader, epoch, writer)\n",
    "    writer.close()\n",
    "start(30, model, DEVICE, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132471c",
   "metadata": {},
   "source": [
    "### 训练过程可视化\n",
    "从上到下，从左往右三图依次是训练集的loss变化、训练集准确率变化、测试集的loss变化、测试集的正确率变化。\n",
    "<img src=\"./images/res_show.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847516f",
   "metadata": {},
   "source": [
    "### 总结\n",
    "* 即使训练集数据量不大，仅仅300张训练集，也可用各种数据增强的方法进行训练，并且模型得到不错的表现\n",
    "* 测试集的正确率基本在第10个epoch(77%)就开始下降了\n",
    "* 训练集的loss在不断下降最终趋于0，训练集准确率到了100%，但测试集的loss值在不断上升，准确率不到80，即过拟合了\n",
    "* 最高一次准确率在81%，是自适应学习率每3个epoch减少10倍，但是lr要有个下限，不然30个epoch会让lr变的非常小\n",
    "* 测试了多个BATCHSIZE, 经测试BATCHSIZE为256时，效果不如小点好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4942c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# jupyterlab里 tab + shift 可以看方法的使用\n",
    "# 测试一下AlexNet\n",
    "res_model = models.alexnet()\n",
    "# summary(res_model, (3, 256, 256))\n",
    "# 最后一层输入4096，输出3\n",
    "res_model.classifier[6] = nn.Linear(res_model.classifier[6].in_features, 3)\n",
    "print(res_model.classifier)\n",
    "# 需要把模型to到Gpu，不然会报错 Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
    "res_model = res_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78d2a5-a153-4cf9-8369-e329e47e63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet测试\n",
    "start(5, res_model, DEVICE, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3abceb1d-8c89-438f-a10b-266b4324c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77ff34-1af8-4117-a3a5-f487ca673a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

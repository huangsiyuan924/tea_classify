{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e6965d",
   "metadata": {},
   "source": [
    "原论文地址：https://www.researchgate.net/publication/323419312_Twelve-layer_deep_convolutional_neural_network_with_stochastic_pooling_for_tea_category_classification_on_GPU_platform\n",
    "\n",
    "鄙人不是人工智能专业的，但是对这个领域非常好奇，最近找到了唐老师的一篇论文，用所学的知识进行了简单的复现。\n",
    "\n",
    "但是由于理论知识和对pytorch使用不够，其中的随机池化（stochastic pooling），目前我无法实现，只能使用torch自带的模块进行模型的搭建。\n",
    "\n",
    "\n",
    "### 开发环境\n",
    "由于本科是做大数据的，经常需要集群，一套大数据服务下来内存动不动占用20G+，所以内存直接给到了32G，但是测试跑深度模型内存占不了太多，重要的是显卡，本人显卡3060 Laptop，功耗130W，6G显存，算力49左右，基本可以跑模型。\n",
    "* 操作系统：win11\n",
    "* cpu：i7-11800H\n",
    "* 显卡：3060 Laptop（6G）\n",
    "* 内存：32G\n",
    "* cuda版本：11.6\n",
    "* python版本：3.6.9\n",
    "* pytorch版本：1.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1535920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from data_augmentation import data_enhance_rotate, data_enhance_gamma\n",
    "# 使用GPU训练，3060显卡，30个epoch一共需要不到半小时\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e219e",
   "metadata": {},
   "source": [
    "### 制作数据集\n",
    "没有原文茶叶数据集，所以我就在kaggle上搜索了相似的花分类数据集\n",
    "\n",
    "kaggle地址在：https://www.kaggle.com/alxmamaev/flowers-recognition?select=flowers\n",
    "\n",
    "其中有五类花共4317个图片，只采用了3类，一共选取900张图片\n",
    "\n",
    "数据集目录树：\n",
    "\n",
    "    datasets\n",
    "\n",
    "    +---test_data\n",
    "    \n",
    "    |   +---daisy\n",
    "    \n",
    "    |   +---dandelion\n",
    "    \n",
    "    |   \\---rose\n",
    "    \n",
    "    \\---train_data\n",
    "    \n",
    "        +---daisy\n",
    "        \n",
    "        +---dandelion\n",
    "        \n",
    "        \\---rose\n",
    "        \n",
    "其中训练集共有300张，三类分别各有100张\n",
    "\n",
    "测试集共有600张，三类各有200张\n",
    "\n",
    "经过数据增强后，训练集共有18300张\n",
    "\n",
    "数据增强函数在data_augmentation.py中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8375aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './datasets'\n",
    "BATCH_SIZE = 256  # 256实测占用4-5G显存\n",
    "\n",
    "data_transforms = {\n",
    "    'train_data': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 均值，标准差\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 均值，标准差\n",
    "    ]),\n",
    "    'test_data': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in\n",
    "                  ['train_data', 'test_data']}\n",
    "# 旋转图片，从-15度到15度，每次递增1，跳过0度，一共产生9000张\n",
    "image_datasets['train_data'] += data_enhance_rotate(data_dir)\n",
    "# 伽马纠正，从0.7到1.3，每次递增0.02，总共30次，一共产生9000张\n",
    "image_datasets['train_data'] += data_enhance_gamma(data_dir)\n",
    "train_loader = torch.utils.data.DataLoader(image_datasets.get(\"train_data\"), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(image_datasets.get(\"test_data\"), batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader一共18300数据，test_loader一共600数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4683fa8",
   "metadata": {},
   "source": [
    "### 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa0e7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./logs\")\n",
    "# 查看第一批的数据集，总共256张，（256, 3, 256, 256）：\n",
    "for imgs, labels in train_loader:\n",
    "    writer.add_images(\"imgs\", imgs)\n",
    "    break\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3bfd67",
   "metadata": {},
   "source": [
    "<img src=\"./images/images_show.png\" alt=\"images_show\" style=\"zoom: 80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cd83d",
   "metadata": {},
   "source": [
    "### 模型搭建\n",
    "按照原文进行模型搭建，随机池化没有实现，就用torch自带的最大池化代替了\n",
    "\n",
    "模型一共1,627,563个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4411acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python_venv\\torch_venv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 40, 86, 86]          1,120\n",
      "├─Conv2d: 1-2                            [-1, 80, 28, 28]          80,080\n",
      "├─Conv2d: 1-3                            [-1, 120, 10, 10]         86,520\n",
      "├─Conv2d: 1-4                            [-1, 120, 10, 10]         129,720\n",
      "├─Conv2d: 1-5                            [-1, 120, 10, 10]         129,720\n",
      "├─Linear: 1-6                            [-1, 100]                 1,200,100\n",
      "├─Dropout: 1-7                           [-1, 100]                 --\n",
      "├─Linear: 1-8                            [-1, 3]                   303\n",
      "==========================================================================================\n",
      "Total params: 1,627,563\n",
      "Trainable params: 1,627,563\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 106.47\n",
      "==========================================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 3.01\n",
      "Params size (MB): 6.21\n",
      "Estimated Total Size (MB): 9.97\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 40, 86, 86]          1,120\n",
       "├─Conv2d: 1-2                            [-1, 80, 28, 28]          80,080\n",
       "├─Conv2d: 1-3                            [-1, 120, 10, 10]         86,520\n",
       "├─Conv2d: 1-4                            [-1, 120, 10, 10]         129,720\n",
       "├─Conv2d: 1-5                            [-1, 120, 10, 10]         129,720\n",
       "├─Linear: 1-6                            [-1, 100]                 1,200,100\n",
       "├─Dropout: 1-7                           [-1, 100]                 --\n",
       "├─Linear: 1-8                            [-1, 3]                   303\n",
       "==========================================================================================\n",
       "Total params: 1,627,563\n",
       "Trainable params: 1,627,563\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 106.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 3.01\n",
       "Params size (MB): 6.21\n",
       "Estimated Total Size (MB): 9.97\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    原文中的网络模型，不过随机池化没有实现\n",
    "\"\"\"\n",
    "class CNN_SP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小\n",
    "        self.conv1 = nn.Conv2d(3, 40, 3, stride=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(40, 80, 5, stride=3, padding=0)\n",
    "        self.conv3 = nn.Conv2d(80, 120, 3, stride=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(120, 120, 3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(120, 120, 3, stride=1, padding=1)\n",
    "        # 原文中dropout的比率为0.1，防止过拟合\n",
    "        self.dropout_layer = torch.nn.Dropout(0.1)\n",
    "        # 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数\n",
    "        self.fc1 = nn.Linear(120 * 10 * 10, 100)\n",
    "        self.fc2 = nn.Linear(100, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 3, 1, 1)\n",
    "        # out = self.pool5(out)\n",
    "        out = out.view(in_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out, dim=1)  # 计算log(softmax(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "# model = StochasticPooling().to(DEVICE)\n",
    "model = CNN_SP().to(DEVICE)\n",
    "# summary(model, (40, 86, 86))\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d2d16",
   "metadata": {},
   "source": [
    "### 自适应学习率，优化器，tensorboard的配置\n",
    "* 学习率：开始0.01，每10个epch就将lr减少10倍，测试了每3个epoch减少一次lr比固定lr准确率高了5%，但是10个epoch减少一次lr不知为何准确率上不去\n",
    "* 优化器：相比较SGDM与Adam，Adam拟合速度比较快，但最终结果差不多\n",
    "* tensorboard：将数据写入logs文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ccbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard, 记录loss和acc\n",
    "writer = SummaryWriter(\"./logs\")\n",
    "start_lr = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=start_lr)\n",
    "\n",
    "'''\n",
    "    自适应学习率，复现原文中的每10个epoch将学习率减少10倍\n",
    "'''\n",
    "def adjust_learning_rate(optimizer, epoch, start_lr):\n",
    "    lr = start_lr * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7373f0",
   "metadata": {},
   "source": [
    "### 训练与测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60568909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    adjust_learning_rate(optimizer, epoch, start_lr)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        # loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tLr:{:.2E}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item(),\n",
    "                optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "            writer.add_scalar('train_loss', loss.item(), (epoch - 1) * len(train_loader) + batch_idx)\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    writer.add_scalar('test_acc', 100. * correct / len(test_loader.dataset), epoch)\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d46419",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9339093",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [2304/18300 (12%)]\tLoss: 1.097219 \tLr:1.00E-02\n",
      "Train Epoch: 1 [4864/18300 (26%)]\tLoss: 1.090676 \tLr:1.00E-02\n",
      "Train Epoch: 1 [7424/18300 (40%)]\tLoss: 1.069286 \tLr:1.00E-02\n",
      "Train Epoch: 1 [9984/18300 (54%)]\tLoss: 0.906102 \tLr:1.00E-02\n",
      "Train Epoch: 1 [12544/18300 (68%)]\tLoss: 0.960054 \tLr:1.00E-02\n",
      "Train Epoch: 1 [15104/18300 (82%)]\tLoss: 0.902408 \tLr:1.00E-02\n",
      "Train Epoch: 1 [17664/18300 (96%)]\tLoss: 0.809350 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 0.8285, Accuracy: 392/600 (65%)\n",
      "\n",
      "Train Epoch: 2 [2304/18300 (12%)]\tLoss: 0.624967 \tLr:1.00E-02\n",
      "Train Epoch: 2 [4864/18300 (26%)]\tLoss: 0.600188 \tLr:1.00E-02\n",
      "Train Epoch: 2 [7424/18300 (40%)]\tLoss: 0.478863 \tLr:1.00E-02\n",
      "Train Epoch: 2 [9984/18300 (54%)]\tLoss: 0.552383 \tLr:1.00E-02\n",
      "Train Epoch: 2 [12544/18300 (68%)]\tLoss: 0.554944 \tLr:1.00E-02\n",
      "Train Epoch: 2 [15104/18300 (82%)]\tLoss: 0.489458 \tLr:1.00E-02\n",
      "Train Epoch: 2 [17664/18300 (96%)]\tLoss: 0.399592 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 0.7292, Accuracy: 444/600 (74%)\n",
      "\n",
      "Train Epoch: 3 [2304/18300 (12%)]\tLoss: 0.338853 \tLr:1.00E-02\n",
      "Train Epoch: 3 [4864/18300 (26%)]\tLoss: 0.352237 \tLr:1.00E-02\n",
      "Train Epoch: 3 [7424/18300 (40%)]\tLoss: 0.327742 \tLr:1.00E-02\n",
      "Train Epoch: 3 [9984/18300 (54%)]\tLoss: 0.212875 \tLr:1.00E-02\n",
      "Train Epoch: 3 [12544/18300 (68%)]\tLoss: 0.188415 \tLr:1.00E-02\n",
      "Train Epoch: 3 [15104/18300 (82%)]\tLoss: 0.324034 \tLr:1.00E-02\n",
      "Train Epoch: 3 [17664/18300 (96%)]\tLoss: 0.620643 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 0.8427, Accuracy: 429/600 (72%)\n",
      "\n",
      "Train Epoch: 4 [2304/18300 (12%)]\tLoss: 0.462232 \tLr:1.00E-02\n",
      "Train Epoch: 4 [4864/18300 (26%)]\tLoss: 0.366939 \tLr:1.00E-02\n",
      "Train Epoch: 4 [7424/18300 (40%)]\tLoss: 0.399144 \tLr:1.00E-02\n",
      "Train Epoch: 4 [9984/18300 (54%)]\tLoss: 0.245538 \tLr:1.00E-02\n",
      "Train Epoch: 4 [12544/18300 (68%)]\tLoss: 0.191642 \tLr:1.00E-02\n",
      "Train Epoch: 4 [15104/18300 (82%)]\tLoss: 0.162637 \tLr:1.00E-02\n",
      "Train Epoch: 4 [17664/18300 (96%)]\tLoss: 0.163442 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 1.2516, Accuracy: 448/600 (75%)\n",
      "\n",
      "Train Epoch: 5 [2304/18300 (12%)]\tLoss: 0.196951 \tLr:1.00E-02\n",
      "Train Epoch: 5 [4864/18300 (26%)]\tLoss: 0.182253 \tLr:1.00E-02\n",
      "Train Epoch: 5 [7424/18300 (40%)]\tLoss: 0.101793 \tLr:1.00E-02\n",
      "Train Epoch: 5 [9984/18300 (54%)]\tLoss: 0.073624 \tLr:1.00E-02\n",
      "Train Epoch: 5 [12544/18300 (68%)]\tLoss: 0.063638 \tLr:1.00E-02\n",
      "Train Epoch: 5 [15104/18300 (82%)]\tLoss: 0.083632 \tLr:1.00E-02\n",
      "Train Epoch: 5 [17664/18300 (96%)]\tLoss: 1.211800 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 1.1084, Accuracy: 393/600 (66%)\n",
      "\n",
      "Train Epoch: 6 [2304/18300 (12%)]\tLoss: 0.514575 \tLr:1.00E-02\n",
      "Train Epoch: 6 [4864/18300 (26%)]\tLoss: 0.442459 \tLr:1.00E-02\n",
      "Train Epoch: 6 [7424/18300 (40%)]\tLoss: 0.301550 \tLr:1.00E-02\n",
      "Train Epoch: 6 [9984/18300 (54%)]\tLoss: 0.277747 \tLr:1.00E-02\n",
      "Train Epoch: 6 [12544/18300 (68%)]\tLoss: 0.218475 \tLr:1.00E-02\n",
      "Train Epoch: 6 [15104/18300 (82%)]\tLoss: 0.176061 \tLr:1.00E-02\n",
      "Train Epoch: 6 [17664/18300 (96%)]\tLoss: 0.129977 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 1.4732, Accuracy: 461/600 (77%)\n",
      "\n",
      "Train Epoch: 7 [2304/18300 (12%)]\tLoss: 0.432843 \tLr:1.00E-02\n",
      "Train Epoch: 7 [4864/18300 (26%)]\tLoss: 0.300809 \tLr:1.00E-02\n",
      "Train Epoch: 7 [7424/18300 (40%)]\tLoss: 0.259566 \tLr:1.00E-02\n",
      "Train Epoch: 7 [9984/18300 (54%)]\tLoss: 0.117622 \tLr:1.00E-02\n",
      "Train Epoch: 7 [12544/18300 (68%)]\tLoss: 0.109836 \tLr:1.00E-02\n",
      "Train Epoch: 7 [15104/18300 (82%)]\tLoss: 0.087582 \tLr:1.00E-02\n",
      "Train Epoch: 7 [17664/18300 (96%)]\tLoss: 0.028560 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 1.4280, Accuracy: 439/600 (73%)\n",
      "\n",
      "Train Epoch: 8 [2304/18300 (12%)]\tLoss: 0.038407 \tLr:1.00E-02\n",
      "Train Epoch: 8 [4864/18300 (26%)]\tLoss: 0.025478 \tLr:1.00E-02\n",
      "Train Epoch: 8 [7424/18300 (40%)]\tLoss: 0.023039 \tLr:1.00E-02\n",
      "Train Epoch: 8 [9984/18300 (54%)]\tLoss: 0.009913 \tLr:1.00E-02\n",
      "Train Epoch: 8 [12544/18300 (68%)]\tLoss: 0.005106 \tLr:1.00E-02\n",
      "Train Epoch: 8 [15104/18300 (82%)]\tLoss: 0.005644 \tLr:1.00E-02\n",
      "Train Epoch: 8 [17664/18300 (96%)]\tLoss: 0.004291 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 2.0407, Accuracy: 447/600 (74%)\n",
      "\n",
      "Train Epoch: 9 [2304/18300 (12%)]\tLoss: 0.003617 \tLr:1.00E-02\n",
      "Train Epoch: 9 [4864/18300 (26%)]\tLoss: 0.001738 \tLr:1.00E-02\n",
      "Train Epoch: 9 [7424/18300 (40%)]\tLoss: 0.019854 \tLr:1.00E-02\n",
      "Train Epoch: 9 [9984/18300 (54%)]\tLoss: 0.007459 \tLr:1.00E-02\n",
      "Train Epoch: 9 [12544/18300 (68%)]\tLoss: 0.029173 \tLr:1.00E-02\n",
      "Train Epoch: 9 [15104/18300 (82%)]\tLoss: 0.022766 \tLr:1.00E-02\n",
      "Train Epoch: 9 [17664/18300 (96%)]\tLoss: 0.008697 \tLr:1.00E-02\n",
      "\n",
      "Test set: Average loss: 1.9628, Accuracy: 461/600 (77%)\n",
      "\n",
      "Train Epoch: 10 [2304/18300 (12%)]\tLoss: 0.002282 \tLr:1.00E-03\n",
      "Train Epoch: 10 [4864/18300 (26%)]\tLoss: 0.003669 \tLr:1.00E-03\n",
      "Train Epoch: 10 [7424/18300 (40%)]\tLoss: 0.000566 \tLr:1.00E-03\n",
      "Train Epoch: 10 [9984/18300 (54%)]\tLoss: 0.002330 \tLr:1.00E-03\n",
      "Train Epoch: 10 [12544/18300 (68%)]\tLoss: 0.003311 \tLr:1.00E-03\n",
      "Train Epoch: 10 [15104/18300 (82%)]\tLoss: 0.001121 \tLr:1.00E-03\n",
      "Train Epoch: 10 [17664/18300 (96%)]\tLoss: 0.001894 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.0243, Accuracy: 458/600 (76%)\n",
      "\n",
      "Train Epoch: 11 [2304/18300 (12%)]\tLoss: 0.000987 \tLr:1.00E-03\n",
      "Train Epoch: 11 [4864/18300 (26%)]\tLoss: 0.001631 \tLr:1.00E-03\n",
      "Train Epoch: 11 [7424/18300 (40%)]\tLoss: 0.002116 \tLr:1.00E-03\n",
      "Train Epoch: 11 [9984/18300 (54%)]\tLoss: 0.002778 \tLr:1.00E-03\n",
      "Train Epoch: 11 [12544/18300 (68%)]\tLoss: 0.000749 \tLr:1.00E-03\n",
      "Train Epoch: 11 [15104/18300 (82%)]\tLoss: 0.001173 \tLr:1.00E-03\n",
      "Train Epoch: 11 [17664/18300 (96%)]\tLoss: 0.001320 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.0913, Accuracy: 457/600 (76%)\n",
      "\n",
      "Train Epoch: 12 [2304/18300 (12%)]\tLoss: 0.000660 \tLr:1.00E-03\n",
      "Train Epoch: 12 [4864/18300 (26%)]\tLoss: 0.001504 \tLr:1.00E-03\n",
      "Train Epoch: 12 [7424/18300 (40%)]\tLoss: 0.001177 \tLr:1.00E-03\n",
      "Train Epoch: 12 [9984/18300 (54%)]\tLoss: 0.001130 \tLr:1.00E-03\n",
      "Train Epoch: 12 [12544/18300 (68%)]\tLoss: 0.003466 \tLr:1.00E-03\n",
      "Train Epoch: 12 [15104/18300 (82%)]\tLoss: 0.000927 \tLr:1.00E-03\n",
      "Train Epoch: 12 [17664/18300 (96%)]\tLoss: 0.003104 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.1379, Accuracy: 456/600 (76%)\n",
      "\n",
      "Train Epoch: 13 [2304/18300 (12%)]\tLoss: 0.001257 \tLr:1.00E-03\n",
      "Train Epoch: 13 [4864/18300 (26%)]\tLoss: 0.001908 \tLr:1.00E-03\n",
      "Train Epoch: 13 [7424/18300 (40%)]\tLoss: 0.001151 \tLr:1.00E-03\n",
      "Train Epoch: 13 [9984/18300 (54%)]\tLoss: 0.002254 \tLr:1.00E-03\n",
      "Train Epoch: 13 [12544/18300 (68%)]\tLoss: 0.000466 \tLr:1.00E-03\n",
      "Train Epoch: 13 [15104/18300 (82%)]\tLoss: 0.001906 \tLr:1.00E-03\n",
      "Train Epoch: 13 [17664/18300 (96%)]\tLoss: 0.001601 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.1743, Accuracy: 456/600 (76%)\n",
      "\n",
      "Train Epoch: 14 [2304/18300 (12%)]\tLoss: 0.000527 \tLr:1.00E-03\n",
      "Train Epoch: 14 [4864/18300 (26%)]\tLoss: 0.001415 \tLr:1.00E-03\n",
      "Train Epoch: 14 [7424/18300 (40%)]\tLoss: 0.000473 \tLr:1.00E-03\n",
      "Train Epoch: 14 [9984/18300 (54%)]\tLoss: 0.001148 \tLr:1.00E-03\n",
      "Train Epoch: 14 [12544/18300 (68%)]\tLoss: 0.000807 \tLr:1.00E-03\n",
      "Train Epoch: 14 [15104/18300 (82%)]\tLoss: 0.001052 \tLr:1.00E-03\n",
      "Train Epoch: 14 [17664/18300 (96%)]\tLoss: 0.000796 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.2068, Accuracy: 454/600 (76%)\n",
      "\n",
      "Train Epoch: 15 [2304/18300 (12%)]\tLoss: 0.000368 \tLr:1.00E-03\n",
      "Train Epoch: 15 [4864/18300 (26%)]\tLoss: 0.000612 \tLr:1.00E-03\n",
      "Train Epoch: 15 [7424/18300 (40%)]\tLoss: 0.001022 \tLr:1.00E-03\n",
      "Train Epoch: 15 [9984/18300 (54%)]\tLoss: 0.000957 \tLr:1.00E-03\n",
      "Train Epoch: 15 [12544/18300 (68%)]\tLoss: 0.000918 \tLr:1.00E-03\n",
      "Train Epoch: 15 [15104/18300 (82%)]\tLoss: 0.001273 \tLr:1.00E-03\n",
      "Train Epoch: 15 [17664/18300 (96%)]\tLoss: 0.000964 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.2453, Accuracy: 452/600 (75%)\n",
      "\n",
      "Train Epoch: 16 [2304/18300 (12%)]\tLoss: 0.000413 \tLr:1.00E-03\n",
      "Train Epoch: 16 [4864/18300 (26%)]\tLoss: 0.001036 \tLr:1.00E-03\n",
      "Train Epoch: 16 [7424/18300 (40%)]\tLoss: 0.002273 \tLr:1.00E-03\n",
      "Train Epoch: 16 [9984/18300 (54%)]\tLoss: 0.000672 \tLr:1.00E-03\n",
      "Train Epoch: 16 [12544/18300 (68%)]\tLoss: 0.001812 \tLr:1.00E-03\n",
      "Train Epoch: 16 [15104/18300 (82%)]\tLoss: 0.002614 \tLr:1.00E-03\n",
      "Train Epoch: 16 [17664/18300 (96%)]\tLoss: 0.000698 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.2781, Accuracy: 453/600 (76%)\n",
      "\n",
      "Train Epoch: 17 [2304/18300 (12%)]\tLoss: 0.000867 \tLr:1.00E-03\n",
      "Train Epoch: 17 [4864/18300 (26%)]\tLoss: 0.000971 \tLr:1.00E-03\n",
      "Train Epoch: 17 [7424/18300 (40%)]\tLoss: 0.001042 \tLr:1.00E-03\n",
      "Train Epoch: 17 [9984/18300 (54%)]\tLoss: 0.001154 \tLr:1.00E-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [12544/18300 (68%)]\tLoss: 0.000325 \tLr:1.00E-03\n",
      "Train Epoch: 17 [15104/18300 (82%)]\tLoss: 0.000488 \tLr:1.00E-03\n",
      "Train Epoch: 17 [17664/18300 (96%)]\tLoss: 0.000628 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.2891, Accuracy: 450/600 (75%)\n",
      "\n",
      "Train Epoch: 18 [2304/18300 (12%)]\tLoss: 0.001390 \tLr:1.00E-03\n",
      "Train Epoch: 18 [4864/18300 (26%)]\tLoss: 0.000301 \tLr:1.00E-03\n",
      "Train Epoch: 18 [7424/18300 (40%)]\tLoss: 0.000283 \tLr:1.00E-03\n",
      "Train Epoch: 18 [9984/18300 (54%)]\tLoss: 0.004392 \tLr:1.00E-03\n",
      "Train Epoch: 18 [12544/18300 (68%)]\tLoss: 0.000198 \tLr:1.00E-03\n",
      "Train Epoch: 18 [15104/18300 (82%)]\tLoss: 0.000510 \tLr:1.00E-03\n",
      "Train Epoch: 18 [17664/18300 (96%)]\tLoss: 0.000564 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.3235, Accuracy: 451/600 (75%)\n",
      "\n",
      "Train Epoch: 19 [2304/18300 (12%)]\tLoss: 0.000306 \tLr:1.00E-03\n",
      "Train Epoch: 19 [4864/18300 (26%)]\tLoss: 0.000792 \tLr:1.00E-03\n",
      "Train Epoch: 19 [7424/18300 (40%)]\tLoss: 0.001067 \tLr:1.00E-03\n",
      "Train Epoch: 19 [9984/18300 (54%)]\tLoss: 0.001001 \tLr:1.00E-03\n",
      "Train Epoch: 19 [12544/18300 (68%)]\tLoss: 0.001125 \tLr:1.00E-03\n",
      "Train Epoch: 19 [15104/18300 (82%)]\tLoss: 0.002891 \tLr:1.00E-03\n",
      "Train Epoch: 19 [17664/18300 (96%)]\tLoss: 0.001220 \tLr:1.00E-03\n",
      "\n",
      "Test set: Average loss: 2.3502, Accuracy: 452/600 (75%)\n",
      "\n",
      "Train Epoch: 20 [2304/18300 (12%)]\tLoss: 0.001024 \tLr:1.00E-04\n",
      "Train Epoch: 20 [4864/18300 (26%)]\tLoss: 0.001537 \tLr:1.00E-04\n",
      "Train Epoch: 20 [7424/18300 (40%)]\tLoss: 0.000545 \tLr:1.00E-04\n",
      "Train Epoch: 20 [9984/18300 (54%)]\tLoss: 0.000716 \tLr:1.00E-04\n",
      "Train Epoch: 20 [12544/18300 (68%)]\tLoss: 0.001008 \tLr:1.00E-04\n",
      "Train Epoch: 20 [15104/18300 (82%)]\tLoss: 0.001034 \tLr:1.00E-04\n",
      "Train Epoch: 20 [17664/18300 (96%)]\tLoss: 0.000150 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3517, Accuracy: 452/600 (75%)\n",
      "\n",
      "Train Epoch: 21 [2304/18300 (12%)]\tLoss: 0.000746 \tLr:1.00E-04\n",
      "Train Epoch: 21 [4864/18300 (26%)]\tLoss: 0.000651 \tLr:1.00E-04\n",
      "Train Epoch: 21 [7424/18300 (40%)]\tLoss: 0.003437 \tLr:1.00E-04\n",
      "Train Epoch: 21 [9984/18300 (54%)]\tLoss: 0.001380 \tLr:1.00E-04\n",
      "Train Epoch: 21 [12544/18300 (68%)]\tLoss: 0.001035 \tLr:1.00E-04\n",
      "Train Epoch: 21 [15104/18300 (82%)]\tLoss: 0.000353 \tLr:1.00E-04\n",
      "Train Epoch: 21 [17664/18300 (96%)]\tLoss: 0.000644 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3540, Accuracy: 450/600 (75%)\n",
      "\n",
      "Train Epoch: 22 [2304/18300 (12%)]\tLoss: 0.000657 \tLr:1.00E-04\n",
      "Train Epoch: 22 [4864/18300 (26%)]\tLoss: 0.000473 \tLr:1.00E-04\n",
      "Train Epoch: 22 [7424/18300 (40%)]\tLoss: 0.000939 \tLr:1.00E-04\n",
      "Train Epoch: 22 [9984/18300 (54%)]\tLoss: 0.001266 \tLr:1.00E-04\n",
      "Train Epoch: 22 [12544/18300 (68%)]\tLoss: 0.000466 \tLr:1.00E-04\n",
      "Train Epoch: 22 [15104/18300 (82%)]\tLoss: 0.000363 \tLr:1.00E-04\n",
      "Train Epoch: 22 [17664/18300 (96%)]\tLoss: 0.000514 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3559, Accuracy: 451/600 (75%)\n",
      "\n",
      "Train Epoch: 23 [2304/18300 (12%)]\tLoss: 0.000833 \tLr:1.00E-04\n",
      "Train Epoch: 23 [4864/18300 (26%)]\tLoss: 0.000235 \tLr:1.00E-04\n",
      "Train Epoch: 23 [7424/18300 (40%)]\tLoss: 0.000631 \tLr:1.00E-04\n",
      "Train Epoch: 23 [9984/18300 (54%)]\tLoss: 0.000782 \tLr:1.00E-04\n",
      "Train Epoch: 23 [12544/18300 (68%)]\tLoss: 0.000465 \tLr:1.00E-04\n",
      "Train Epoch: 23 [15104/18300 (82%)]\tLoss: 0.001370 \tLr:1.00E-04\n",
      "Train Epoch: 23 [17664/18300 (96%)]\tLoss: 0.000462 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3566, Accuracy: 450/600 (75%)\n",
      "\n",
      "Train Epoch: 24 [2304/18300 (12%)]\tLoss: 0.000325 \tLr:1.00E-04\n",
      "Train Epoch: 24 [4864/18300 (26%)]\tLoss: 0.000587 \tLr:1.00E-04\n",
      "Train Epoch: 24 [7424/18300 (40%)]\tLoss: 0.000962 \tLr:1.00E-04\n",
      "Train Epoch: 24 [9984/18300 (54%)]\tLoss: 0.000584 \tLr:1.00E-04\n",
      "Train Epoch: 24 [12544/18300 (68%)]\tLoss: 0.000466 \tLr:1.00E-04\n",
      "Train Epoch: 24 [15104/18300 (82%)]\tLoss: 0.000846 \tLr:1.00E-04\n",
      "Train Epoch: 24 [17664/18300 (96%)]\tLoss: 0.000391 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3591, Accuracy: 450/600 (75%)\n",
      "\n",
      "Train Epoch: 25 [2304/18300 (12%)]\tLoss: 0.001401 \tLr:1.00E-04\n",
      "Train Epoch: 25 [4864/18300 (26%)]\tLoss: 0.000639 \tLr:1.00E-04\n",
      "Train Epoch: 25 [7424/18300 (40%)]\tLoss: 0.000330 \tLr:1.00E-04\n",
      "Train Epoch: 25 [9984/18300 (54%)]\tLoss: 0.001660 \tLr:1.00E-04\n",
      "Train Epoch: 25 [12544/18300 (68%)]\tLoss: 0.000475 \tLr:1.00E-04\n",
      "Train Epoch: 25 [15104/18300 (82%)]\tLoss: 0.000507 \tLr:1.00E-04\n",
      "Train Epoch: 25 [17664/18300 (96%)]\tLoss: 0.000453 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3600, Accuracy: 449/600 (75%)\n",
      "\n",
      "Train Epoch: 26 [2304/18300 (12%)]\tLoss: 0.000462 \tLr:1.00E-04\n",
      "Train Epoch: 26 [4864/18300 (26%)]\tLoss: 0.001772 \tLr:1.00E-04\n",
      "Train Epoch: 26 [7424/18300 (40%)]\tLoss: 0.001300 \tLr:1.00E-04\n",
      "Train Epoch: 26 [9984/18300 (54%)]\tLoss: 0.000458 \tLr:1.00E-04\n",
      "Train Epoch: 26 [12544/18300 (68%)]\tLoss: 0.000521 \tLr:1.00E-04\n",
      "Train Epoch: 26 [15104/18300 (82%)]\tLoss: 0.001024 \tLr:1.00E-04\n",
      "Train Epoch: 26 [17664/18300 (96%)]\tLoss: 0.001051 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3633, Accuracy: 449/600 (75%)\n",
      "\n",
      "Train Epoch: 27 [2304/18300 (12%)]\tLoss: 0.000975 \tLr:1.00E-04\n",
      "Train Epoch: 27 [4864/18300 (26%)]\tLoss: 0.000169 \tLr:1.00E-04\n",
      "Train Epoch: 27 [7424/18300 (40%)]\tLoss: 0.000472 \tLr:1.00E-04\n",
      "Train Epoch: 27 [9984/18300 (54%)]\tLoss: 0.000648 \tLr:1.00E-04\n",
      "Train Epoch: 27 [12544/18300 (68%)]\tLoss: 0.000974 \tLr:1.00E-04\n",
      "Train Epoch: 27 [15104/18300 (82%)]\tLoss: 0.000657 \tLr:1.00E-04\n",
      "Train Epoch: 27 [17664/18300 (96%)]\tLoss: 0.000292 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3654, Accuracy: 451/600 (75%)\n",
      "\n",
      "Train Epoch: 28 [2304/18300 (12%)]\tLoss: 0.000852 \tLr:1.00E-04\n",
      "Train Epoch: 28 [4864/18300 (26%)]\tLoss: 0.002082 \tLr:1.00E-04\n",
      "Train Epoch: 28 [7424/18300 (40%)]\tLoss: 0.000808 \tLr:1.00E-04\n",
      "Train Epoch: 28 [9984/18300 (54%)]\tLoss: 0.000805 \tLr:1.00E-04\n",
      "Train Epoch: 28 [12544/18300 (68%)]\tLoss: 0.000585 \tLr:1.00E-04\n",
      "Train Epoch: 28 [15104/18300 (82%)]\tLoss: 0.000565 \tLr:1.00E-04\n",
      "Train Epoch: 28 [17664/18300 (96%)]\tLoss: 0.000159 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3669, Accuracy: 451/600 (75%)\n",
      "\n",
      "Train Epoch: 29 [2304/18300 (12%)]\tLoss: 0.000581 \tLr:1.00E-04\n",
      "Train Epoch: 29 [4864/18300 (26%)]\tLoss: 0.001014 \tLr:1.00E-04\n",
      "Train Epoch: 29 [7424/18300 (40%)]\tLoss: 0.000724 \tLr:1.00E-04\n",
      "Train Epoch: 29 [9984/18300 (54%)]\tLoss: 0.000292 \tLr:1.00E-04\n",
      "Train Epoch: 29 [12544/18300 (68%)]\tLoss: 0.000853 \tLr:1.00E-04\n",
      "Train Epoch: 29 [15104/18300 (82%)]\tLoss: 0.000546 \tLr:1.00E-04\n",
      "Train Epoch: 29 [17664/18300 (96%)]\tLoss: 0.000566 \tLr:1.00E-04\n",
      "\n",
      "Test set: Average loss: 2.3699, Accuracy: 449/600 (75%)\n",
      "\n",
      "Train Epoch: 30 [2304/18300 (12%)]\tLoss: 0.000877 \tLr:1.00E-05\n",
      "Train Epoch: 30 [4864/18300 (26%)]\tLoss: 0.000531 \tLr:1.00E-05\n",
      "Train Epoch: 30 [7424/18300 (40%)]\tLoss: 0.003624 \tLr:1.00E-05\n",
      "Train Epoch: 30 [9984/18300 (54%)]\tLoss: 0.000412 \tLr:1.00E-05\n",
      "Train Epoch: 30 [12544/18300 (68%)]\tLoss: 0.001050 \tLr:1.00E-05\n",
      "Train Epoch: 30 [15104/18300 (82%)]\tLoss: 0.000485 \tLr:1.00E-05\n",
      "Train Epoch: 30 [17664/18300 (96%)]\tLoss: 0.001094 \tLr:1.00E-05\n",
      "\n",
      "Test set: Average loss: 2.3700, Accuracy: 449/600 (75%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, epoch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132471c",
   "metadata": {},
   "source": [
    "### 训练过程可视化\n",
    "从上到下，从左往右三图依次是训练集的loss变化、测试集的正确率变化、测试集的loss变化\n",
    "<img src=\"./images/res_show.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847516f",
   "metadata": {},
   "source": [
    "### 总结\n",
    "#### 1. 测试集的正确率基本在第10个epoch(76%)就开始下降了, 而此时lr为0.001\n",
    "#### 2. 训练集的loss在不断下降，但测试集的loss值在不断上升，推测觉得是数据集太少或者数据集处理不当导致的，即过拟合了\n",
    "#### 3. 最高一次准确率在81%，是自适应学习率每3个epoch减少10倍，但是要有个下限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942c7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
